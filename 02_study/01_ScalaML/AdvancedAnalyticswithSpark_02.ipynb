{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Introduction to Data Analysis with Scala and Spark\n",
    "\n",
    "- 데이터 클린징은 데이터 사이언스 프로젝트에서 첫번째 단계이며, 매우 중요한 단계임.\n",
    "- 현명한 분석가는 기초 데이터품질에 문제가 있거나,  편의 또는 리얼 데이터가 아니라고 판단되어지만 데이터분석을 취소함.\n",
    "\n",
    "\n",
    "- 이렇게 중요함에도 많은 교재와 강의에서 데이터 클린징은 언급되지 않거나 데이터 주어진 것으로 넘어감.\n",
    "- 데이터 클린징은 정말 지루하고 지루하고 지루한 작업.\n",
    "- 많은 새로운 데이터 과학자들은 최소한의 허용된 상태의 데이터만을 가지고 분석을 바로 시작해서 엉뚱한 결과가 얻은 후에 데이터 품질에 심각한 문제가 있다라고 인식함.\n",
    "\n",
    "\n",
    "- garbage in, garbage out\n",
    "\n",
    "\n",
    "- 데이터 과학자은 재미있고 가치있는 문제을 발견할 수 있는 능력이 필요\n",
    "\n",
    "\n",
    "- ML 알고리즘, 실시간분석, Web-Scale 그래프분석을 할 수 있는 Spark가 데이터 클린징과 가치있는 문제를 발견할 수 있는데 도움이 될것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scala for Data Scientists\n",
    "- Spark 는  R 또는 Python에서 사용이 가능하고, PySpark도 있음.\n",
    "- 그러나, Spark 프레임워크와 동일한 언어로 Spark을 학습하는것이 많은 이점이 있어서 중요한 예제들은 Scala로 작성됨.\n",
    "     - 성능의 오버헤드를 감소\n",
    "     - 최신버전에서 가장 좋은 접근방법을 제공\n",
    "     - Spark 철학을 이해하는데 도움."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Spark Programming Model\n",
    "- Spark program을 작성하는 일반적인 요소\n",
    "    - 1) 입력데이터 셋에서  필요한 형식으로 데이터 변환을 정의\n",
    "    - 2) 변환된 데이터를 저장소 또는 메모리로 결과를 출력하는 action을 포함.\n",
    "    - 3) 분산방식으로 얻어진 결과를  운영하기 위한  로컬 컴퓨팅.\n",
    "    \n",
    "- Spark는  저장소와 실행에 대한 2가지 추상화 프레임워크 사이를 상호동작을 제공함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and installing Spark 1.5.2\n",
    "\n",
    "### Java Development Kit 7 (JDK) 설치\n",
    "- http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html \n",
    "- sudo apt-get install openjdk-7-jdk\n",
    "\n",
    "### download the latest release of Spark\n",
    "- https://spark.apache.org/downloads.html\n",
    "- Pre-built for Hadoop 2.6 and later과 Direct Download을 선택\n",
    "- wget http://apache.mirror.cdnetworks.com/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz\n",
    "- tar xvf spark-1.5.2-bin-hadoop2.6.tgz\n",
    "- ln -s spark-1.5.2-bin-hadoop2.6 spark\n",
    "- cd spark \n",
    "- ls \n",
    "\n",
    "### Spark의 내부 디렉토리 구조\n",
    "- core : sparkAPI와 핵심 구성요소의 소스코드가 포함됨.\n",
    "- bin : spark app과 spark shell을 실행시키기 위한 실행파일들이 포함됨.\n",
    "- graphx, mllib, sql, streaming : graph processing, machine learning, queries, and stream processing에 관련된 LIbrary\n",
    "- example : spark app의 예제와 데모를 포함.\n",
    "- conf : slave 노드와 기타 설정파일이 포함.\n",
    "\n",
    "### Spark 경로 설정\n",
    "- ~/.bashrc  or ~/.bash_profile\n",
    "```\n",
    "export SPARK_HOME=\"/home/deepbio/app/spark/\"\n",
    "```\n",
    "\n",
    "- source ~/.bashrc or ~/.bash_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started: The Spark Shell and SparkContext\n",
    "\n",
    "- UC Irvine Machine Learning Repository에서부터 얻은 샘플데이터를 사용함.\n",
    "- 이 데이터는 2010년에 German hospital에서 수행한 record linkage 연구에서 설계되어짐.\n",
    "- 환자 이름, 주소, 생년월일 같은 정보로 환자 의료 기록들을 매칭함.\n",
    "- 매칭한 필드는 얼마큼 비슷한지를 0.0 ~ 1.0 사이의 숫자값으로 할당됨.  수작업으로 이루어짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "$ mkdir linkage\n",
    "$ cd linkage/\n",
    "$ curl -o donation.zip http://bit.ly/1Aoywaq\n",
    "$ unzip donation.zip\n",
    "$ unzip 'block_*.zip'\n",
    "\n",
    "\n",
    "$ hadoop fs -mkdir linkage\n",
    "$ hadoop fs -put block_*.csv linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 256400\r\n",
      "drwxrwxr-x 2 deepbio deepbio     4096 Dec 16 18:22 .\r\n",
      "drwxr-xr-x 6 deepbio deepbio     4096 Dec 16 18:22 ..\r\n",
      "-rw-r--r-- 1 deepbio deepbio 26255957 Mar  9  2011 block_10.csv\r\n",
      "-rw-r--r-- 1 deepbio deepbio 26248574 Mar  9  2011 block_1.csv\r\n",
      "-rw-r--r-- 1 deepbio deepbio 26241784 Mar  9  2011 block_2.csv\r\n",
      "-rw-r--r-- 1 deepbio deepbio 26253247 Mar  9  2011 block_3.csv\r\n",
      "-rw-r--r-- 1 deepbio deepbio 26247471 Mar  9  2011 block_4.csv\r\n",
      "-rw-r--r-- 1 deepbio deepbio 26249424 Mar  9  2011 block_5.csv\r\n",
      "-rw-r--r-- 1 deepbio deepbio 26256126 Mar  9  2011 block_6.csv\r\n",
      "-rw-r--r-- 1 deepbio deepbio 26261911 Mar  9  2011 block_7.csv\r\n",
      "-rw-r--r-- 1 deepbio deepbio 26253911 Mar  9  2011 block_8.csv\r\n",
      "-rw-r--r-- 1 deepbio deepbio 26254012 Mar  9  2011 block_9.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls -al ~/note/work/linkage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"\r\n",
      "37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE\r\n",
      "39086,47614,1,?,1,?,1,1,1,1,1,TRUE\r\n",
      "70031,70237,1,?,1,?,1,1,1,1,1,TRUE\r\n",
      "84795,97439,1,?,1,?,1,1,1,1,1,TRUE\r\n",
      "36950,42116,1,?,1,1,1,1,1,1,1,TRUE\r\n",
      "42413,48491,1,?,1,?,1,1,1,1,1,TRUE\r\n",
      "25965,64753,1,?,1,?,1,1,1,1,1,TRUE\r\n",
      "49451,90407,1,?,1,?,1,1,1,1,0,TRUE\r\n",
      "39932,40902,1,?,1,?,1,1,1,1,1,TRUE\r\n"
     ]
    }
   ],
   "source": [
    "! head ~/note/work/linkage/block_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hadoop 2.6 + spark에서 할때는 \n",
    "bin/hdfs dfsadmin -report\n",
    "bin/yarn node -list\n",
    "\n",
    "bin/hdfs dfs  -mkdir /linkage\n",
    "bin/hdfs dfs -put ~/linkage/*  /linkage\n",
    "bin/hdfs dfs -ls  /linkage\n",
    "bin/hdfs dfs  -tail  /linkage/block_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spark 설치 및 실행은 다른 문서로 대치함.\n",
    "- linkage 데이터를 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val 과 var 의 비교 설명\n",
    "val rawblocks = sc.textFile(\"linkage\")\n",
    "rawblocks = sc.textFile(\"linkage\")\n",
    "\n",
    "var varblocks = sc.textFile(\"linkage\")\n",
    "varblocks = sc.textFile(\"linkage\")\n",
    "\n",
    "val rawblocks = sc.textFile(\"linakge\")\n",
    "val rawblocks = sc.textFile(\"linkage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing Data from the Cluster to the Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark shell 시작\n",
    "bin/spark-shell  --driver-memory 2g\n",
    "\n",
    "# 하둡\n",
    "val rawblocks = sc.textFile(\"hdfs://node01:9000/linkage\")\n",
    "\n",
    "# local mode\n",
    "val rawblocks = sc.textFile(\"linkage\")\n",
    "rawblocks.first\n",
    "\n",
    "# 전체데이터에서 10개만 데이터를 로컬로 가지고 옴.\n",
    "val head = rawblocks.take(10)\n",
    "head.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - scala에서는 타임을 compiler가 예상할 수 있으면 생략 가능,  축약된 함수정의와 잘 정의된 함수정의를 확인. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isHeader(line: String) = line.contains(\"id_1\")\n",
    "\n",
    "def isHeader(line: String): Boolean = {\n",
    "    line.contains(\"id_1\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 헤더정보를 출력해보고, 헤더가 아닌 정보도 출력하는 2가지 방법을 알아보자.\n",
    "- scala의 람다표현식으로 filter는 하나의 인자를 받아서 true와 false을 리턴하는 함수를 인자로 받음.\n",
    "- isHeader가 하나의 인자만을 필요하기 때문에 타임을 예상할 수 있어서 생략가능함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head.filter(isHeader).foreach(println)\n",
    "head.filter(x => isHeader(x)).length\n",
    "\n",
    "head.filterNot(isHeader).length\n",
    "head.filter(x => !isHeader(x)).length\n",
    "head.filter(!isHeader(_)).length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shipping Code from the Client to the Cluster\n",
    "\n",
    "- spark에서는 cluster에서 동작하는 코드나 local machine에서 동작하는 코드나 정확히 일치함.\n",
    "- 아주 강력함을 제공\n",
    "    - 대화형식으로 개발이 가능함.\n",
    "    - 샘플데이터 셋으로 개발한 후에 실제 데이터로 적용이 바로 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 실제 데이터에서 동작시킴.\n",
    "val noheader = rawblocks.filter(x => !isHeader(x))\n",
    "\n",
    "noheader.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring Data with Tuples and Case Classes\n",
    "- scala에서는 python과 같이 튜플 타임을 제공함.\n",
    "- 또한, case classes을 제공해서 record 정보를 편리하게 사용할 수 있게 함.\n",
    "     - immutable class 한 단순한 클래스\n",
    "     - 자바와 같이 toString, equals, hashCode와 같은 함수를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터를 가공하는 기본적인 방법들을 소개함.\n",
    "val line = head(5)\n",
    "val pieces = line.split(',')\n",
    "val id1 = pieces(0).toInt\n",
    "val id2 = pieces(1).toInt\n",
    "val matched = pieces(11).toBoolean\n",
    "val rawscores = pieces.slice(2, 11)\n",
    "\n",
    "# 에러 발생함.\n",
    "rawscores.map(s => s.toDouble)\n",
    "\n",
    "#에러에 대한 처리함.\n",
    "def toDouble(s: String) = {\n",
    "   if (\"?\".equals(s)) Double.NaN else s.toDouble\n",
    "}\n",
    "val scores = rawscores.map(toDouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 튜플 형식으로 데이터를 가공\n",
    "\n",
    "def parse(line: String) = {\n",
    "  val pieces = line.split(',')\n",
    "  val id1 = pieces(0).toInt\n",
    "  val id2 = pieces(1).toInt\n",
    "  val scores = pieces.slice(2, 11).map(toDouble)\n",
    "  val matched = pieces(11).toBoolean\n",
    "  (id1, id2, scores, matched)\n",
    "}\n",
    "val tup = parse(line)\n",
    "tup._1\n",
    "tup.productElement(0)\n",
    "tup.productArity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# case class 형식으로 데이터를 가공\n",
    "case class MatchData(id1: Int, id2: Int, scores: Array[Double], matched: Boolean)\n",
    "\n",
    "def parse(line: String) = {\n",
    "  val pieces = line.split(',')\n",
    "  val id1 = pieces(0).toInt\n",
    "  val id2 = pieces(1).toInt\n",
    "  val scores = pieces.slice(2, 11).map(toDouble)\n",
    "  val matched = pieces(11).toBoolean\n",
    "  MatchData(id1, id2, scores, matched)\n",
    "}\n",
    "val md = parse(line)\n",
    "md.matched\n",
    "md.id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# client내부의 데이터로 확인\n",
    "val mds = head.filter(x => !isHeader(x)).map(x => parse(x))\n",
    "mds(0)\n",
    "\n",
    "# 실제 데이터로 cluster에서 동작시킴.\n",
    "val parsed = noheader.map(line => parse(line))\n",
    "parsed.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RDD 데이터를 메모리에 캐싱할 수 있도록 cache함수를 제공함.\n",
    "# 자주 사용하는 데이터는 cache하는 것이 좋음.\n",
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터를 그룹핑하고 그룹별 합산연산을 수행\n",
    "- groupBy()함수를 통해서  Scala Map[Boolean, Array[MatchData]]에 true인 경우와 false 인 경우가 나누어짐.\n",
    "- mapValues()함수에서  true경우의 사이즈와 false 경우의 사이즈값을 출력함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val grouped = mds.groupBy(md => md.matched)\n",
    "grouped.mapValues(x => x.size).foreach(println)\n",
    "\n",
    "val groupedRDD = parsed.groupBy(md => md.matched)\n",
    "groupedRDD.mapValues(x => x.size).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클러스터상에서 aggregation을 수행할때는 여러개의 머신에서 수행된다는 것을 기억해야 함.\n",
    "- aggregations 작업은 각각 머신에서 네트워크를 통해서 데이터 이동이 필수임.\n",
    "- 데이터 이동은 데이터의 serializing, compressing, sending, decompressing, deserializing와 같은 많은 작업을 수행함.\n",
    "- 이와 같은 작업을 빠르게 처리하기 위해서는 데이터의 이동을 최소화함.\n",
    "- aggregations전에 필요한 데이터만을 filtering을 처리함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Histograms\n",
    "- MatchData 레코드들이 field 필드에 대해서 true와 falsed의 개수가 얼마큼인지 세어보자.\n",
    "- RDD[T] 에서는 countByValue()함수에서 Map[T,Long]  형식으로 리턴함.\n",
    "\n",
    "- Map[T,Long]은  sort 함수를 제공하지 않아서 Scala의 Seq type으로 변환함.\n",
    "- Scala은 Seq은 자바의 List 인터페이스와 비슷함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val matchCounts = parsed.map(md => md.matched).countByValue()\n",
    "val matchCountsSeq = matchCounts.toSeq\n",
    "\n",
    "matchCountsSeq.sortBy(_._1).foreach(println)\n",
    "matchCountsSeq.sortBy(_._2).foreach(println)\n",
    "matchCountsSeq.sortBy(_._2).reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 참고 : Word Count 예제로  map()함수의 기능을 알아봄.\n",
    "val textFile = spark.textFile(\"hdfs://...\")\n",
    "val counts = textFile.flatMap(line => line.split(\" \"))\n",
    "                 .map(word => (word, 1))\n",
    "                 .reduceByKey(_ + _)\n",
    "counts.saveAsTextFile(\"hdfs://...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 참고 : hadoop MapReduce로  WordCount 구현\n",
    "public class WordCount {\n",
    "\n",
    "  public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(Object key, Text value, Context context\n",
    "                    ) throws IOException, InterruptedException {\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        word.set(itr.nextToken());\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    public void reduce(Text key, Iterable<IntWritable> values,\n",
    "                       Context context\n",
    "                       ) throws IOException, InterruptedException {\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "    job.setJarByClass(WordCount.class);\n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics for Continuous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Patient records에 있는 match score와 연속형 변수에 대해서,  평균, 표준편차, 최대값, 최소값 같은 분표에 대한 통계치를 사용함.\n",
    "- RDD[Double]에서는 state()함수를 지원하여 요약통계량을 구해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 결측치가 포함\n",
    "parsed.map(md => md.scores(0)).stats()\n",
    "\n",
    "# 결측치 처리\n",
    "import java.lang.Double.isNaN\n",
    "parsed.map(md => md.scores(0)).filter(!isNaN(_)).stats()\n",
    "\n",
    "# 9개의 score에 대해서 일괄처리함.\n",
    "val stats = (0 until 9).map(i => {\n",
    "  parsed.map(md => md.scores(i)).filter(!isNaN(_)).stats()\n",
    "})\n",
    "\n",
    "stats(1)\n",
    "stats(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Reusable Code for Computing Summary Statistics\n",
    "\n",
    "- Spark에서는 요약통계량을 계산하고, 유연하게 사용할 수 있는 유틸클래스를 제공함\n",
    "- org.apache.spark.util.StatCounter\n",
    "- 이 StatCounter 클래스는 결측치( NaN )값을 처리하지 못하므로 이것을 처리할 수 있는 클래스를 만들어보자.\n",
    "- Serializable 을 상속받은 이유는 spark RDD 내부에서 인스턴스로 사용하기 위함.  \n",
    "- RDD내부에 포함되는 인스턴스가 Serializable을 상속받지 않으면 에러가 발생함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.util.StatCounter\n",
    "class NAStatCounter extends Serializable {\n",
    "    val stats: StatCounter = new StatCounter()\n",
    "    var missing: Long = 0\n",
    "    \n",
    "    def add(x: Double): NAStatCounter = {\n",
    "        if (java.lang.Double.isNaN(x)) {\n",
    "            missing += 1\n",
    "        } else {\n",
    "            stats.merge(x)\n",
    "        }\n",
    "        this\n",
    "    }\n",
    "    \n",
    "    def merge(other: NAStatCounter): NAStatCounter = {\n",
    "         stats.merge(other.stats)\n",
    "         missing += other.missing\n",
    "         this\n",
    "    }\n",
    "    override def toString = {\n",
    "        \"stats: \" + stats.toString + \" NaN: \" + missing\n",
    "    }\n",
    "}\n",
    "\n",
    "object NAStatCounter extends Serializable {\n",
    "   def apply(x: Double) = new NAStatCounter().add(x)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  scala 파일로 만들어 놓은 class를 로드 \n",
    ":load StatsWithMissing.scala\n",
    "\n",
    "# object의 apply() 함수 호출\n",
    "val nastats = NAStatCounter.apply(17.29)\n",
    "\n",
    "# NAStatCounter의 생성자는 없지만, apply()함수가 있어서 Double타입의 변수에 적용됨.\n",
    "val nastats = NAStatCounter(17.29)\n",
    "\n",
    "\n",
    "val nas1 = NAStatCounter(10.0)\n",
    "nas1.add(2.1)\n",
    "val nas2 = NAStatCounter(Double.NaN)\n",
    "nas1.merge(nas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val arr = Array(1.0, Double.NaN, 17.29)\n",
    "val nas = arr.map(d => NAStatCounter(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parsed RDD내부에 있는 레코드에는 Array[Double] 있고, 이것을 Array[NAStatCounter] 형식으로 변환하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nasRDD = parsed.map(md => {\n",
    "    md.scores.map(d => NAStatCounter(d))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scala의 zip()함수를 이용해서 2개의 배열을 조합할 수 있음.\n",
    "- zip() 함수의 기능을 예제로 확인함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nas1 = Array(1.0,        Double.NaN).map(d => NAStatCounter(d))\n",
    "val nas2 = Array(Double.NaN, 2.0       ).map(d => NAStatCounter(d))\n",
    "nas1.zip(nas2)\n",
    "val merged = nas1.zip(nas2).map(p => p._1.merge(p._2))\n",
    "\n",
    "# 튜플에 대해서 case() 문을 사용해서 편리하게 사용할 수 있음.\n",
    "val merged1 = nas1.zip(nas2).map { case (a, b) => a.merge(b) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scala의 reduce()함수는 T 타임의 2개의 인자를 받아서  2개의 인자를 가공해서 T타임의 값을 리턴해줌.\n",
    "- collection객체 내부에 있는 모든 요소들에 대해서 반복적으로 수행함.\n",
    "- 예) a = a + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nas = List(nas1, nas2)\n",
    "val merged2 = nas.reduce((n1, n2) => {\n",
    "    n1.zip(n2).map { case (a, b) => a.merge(b) }\n",
    "})\n",
    "\n",
    "\n",
    "val reduced = nasRDD.reduce((n1, n2) => {\n",
    "    n1.zip(n2).map { case (a, b) => a.merge(b) }\n",
    "})\n",
    "reduced.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StatsWithMissing.scala 에 추가해서  모듈화함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "def statsWithMissing(rdd: RDD[Array[Double]]): Array[NAStatCounter] = {\n",
    "    val nastats = rdd.mapPartitions((iter: Iterator[Array[Double]]) => {\n",
    "        val nas: Array[NAStatCounter] = iter.next().map(d => NAStatCounter(d))\n",
    "        iter.foreach(arr => {\n",
    "            nas.zip(arr).foreach { case (n, d) => n.add(d) }\n",
    "        })\n",
    "        Iterator(nas)\n",
    "    })\n",
    "    nastats.reduce((n1, n2) => {\n",
    "        n1.zip(n2).map { case (a, b) => a.merge(b) }\n",
    "    })\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Variable Selection and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- statsWithMissing()함수를 이용해서  matches와 nonmatches 의 2가지 경우에 대해서 데이터의 분포가 어떻게 다른지 확인하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val statsm = statsWithMissing(parsed.filter(_.matched).map(_.scores))\n",
    "val statsn = statsWithMissing(parsed.filter(!_.matched).map(_.scores))\n",
    "\n",
    "# 위의 코드와 동일한 기능을 함.\n",
    "val matchedRDD = parsed.filter( a =>  a.matched == true ).map( b =>  b.scores)\n",
    "val statsm = statsWithMissing(  matchedRDD  )\n",
    "\n",
    "val notMatchedRDD = parsed.filter( a =>  a.matched != true ).map( b =>  b.scores)\n",
    "val statsn = statsWithMissing( notMatchedRDD )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matches와 nonmatches 일때의 데이터의 분포를 구별하기 위해서 평균의 차이를 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statsm.zip(statsn).map { case(m, n) =>\n",
    "    (m.missing + n.missing, m.stats.mean - n.stats.mean)\n",
    "}.foreach(println)\n",
    "\n",
    "((1007, 0.2854...), 0)\n",
    "((5645434,0.09104268062279874), 1)\n",
    "((0,0.6838772482597568), 2)\n",
    "((5746668,0.8064147192926266), 3)\n",
    "((0,0.03240818525033484), 4)\n",
    "((795,0.7754423117834044), 5)\n",
    "((795,0.5109496938298719), 6)\n",
    "((795,0.7762059675300523), 7)\n",
    "((12843,0.9563812499852178), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 통계량에서 데이터에서 2가지 중요한 특징을 찾을 수 있음\n",
    "    - 중요한 차이가 어디에서 발생하고 있는지\n",
    "    - 의미있는 값을 나타날수 있는 데이터가 무엇인지\n",
    "- 1번인 경우에는  missing 데이터가 많고 차이가 상대적으로 적어서 유용해보이지 않음.\n",
    "- 4번인 경우도 도움이 되지 않을 것 같음.\n",
    "- 5번, 7번인 경우는 훌륭한 데이터로 보여짐.\n",
    "- 2, 6, 8번은 유익해보임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 좋은 특성치인 2, 5, 6, 7, 8번에 대해서 값의 합을 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naz(d: Double) = if (Double.NaN.equals(d)) 0.0 else d\n",
    "\n",
    "case class Scored(md: MatchData, score: Double)\n",
    "\n",
    "val ct = parsed.map(md => {\n",
    "    val score = Array(2, 5, 6, 7, 8).map(i => naz(md.scores(i))).sum\n",
    "    Scored(md, score)\n",
    "})\n",
    "\n",
    "ct.filter(s => s.score >= 4.0).map(s => s.md.matched).countByValue()\n",
    "Map(false -> 637, true -> 20871)\n",
    "\n",
    "ct.filter(s => s.score >= 2.0).map(s => s.md.matched).countByValue()\n",
    "Map(false -> 596414, true -> 20931)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- score값을 조절해서 true match 된 자료를 필터링해서 분석에 사용할 수 있음.\n",
    "\n",
    "\n",
    "\n",
    "- 다음장에서는 Spark MLlib을 다룸."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
